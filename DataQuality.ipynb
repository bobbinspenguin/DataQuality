{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac404fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533a69f",
   "metadata": {},
   "source": [
    "# Custom Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_analysis_exception(exception: AnalysisException) -> dict:\n",
    "    \"\"\"Analyzes a PySpark AnalysisException to extract details.\n",
    "\n",
    "    This function parses the error message from a PySpark `AnalysisException` to identify common\n",
    "    causes of Spark SQL failures, such as missing columns, missing tables, ambiguous columns,\n",
    "    missing paths, data type mismatches, and schema mismatches. It returns a dictionary with\n",
    "    standardized fields for error type, a short name, a human-readable description, extracted\n",
    "    details, the raw error message, and the stacktrace.\n",
    "\n",
    "    Args:\n",
    "        exception (AnalysisException): The PySpark AnalysisException instance to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - error_type (str): A standardized error category (e.g., 'MissingColumnError').\n",
    "            - short_name (str): A short, code-friendly error name (e.g., 'ColumnNotFound').\n",
    "            - description (str): A human-readable description of the error.\n",
    "            - details (dict): Extracted details relevant to the error type (e.g., column or table names).\n",
    "            - raw_message (str): The original error message from the exception.\n",
    "            - stacktrace (str): The stacktrace at the point of exception analysis.\n",
    "\n",
    "    Example:\n",
    "        try:\n",
    "            df.select(\"non_existent_column\").show()\n",
    "        except AnalysisException as e:\n",
    "            info = analyze_analysis_exception(e)\n",
    "            print(info[\"description\"])\n",
    "            # Output: \"Column 'non_existent_column' not found in DataFrame\"\n",
    "\n",
    "    Notes:\n",
    "        - The function uses regex to extract specific error details from known AnalysisException patterns.\n",
    "        - If the error does not match a known pattern, a generic error type and description are returned.\n",
    "        - This function is intended to support robust error logging and user-friendly diagnostics in\n",
    "          data quality and validation workflows.\n",
    "    \"\"\"\n",
    "    error_message = str(exception)\n",
    "    result = {\n",
    "        \"error_type\": \"UnknownAnalysisError\",\n",
    "        \"short_name\": \"AnalysisException\",\n",
    "        \"description\": \"An error occurred during Spark SQL analysis\",\n",
    "        \"details\": {},\n",
    "        \"raw_message\": error_message,\n",
    "        \"stacktrace\": traceback.format_exc()\n",
    "    }\n",
    "    \n",
    "    # Extract column name from \"cannot resolve '`column_name`' given input columns: [col1, col2, ...]\"\n",
    "    if \"cannot resolve\" in error_message and \"given input columns\" in error_message:\n",
    "        result[\"error_type\"] = \"MissingColumnError\"\n",
    "        result[\"short_name\"] = \"ColumnNotFound\"\n",
    "        \n",
    "        # Extract the unresolved column name\n",
    "        unresolved_match = re.search(r\"cannot resolve '(?:`)?([^'`]+)(?:`)?'\", error_message)\n",
    "        unresolved_column = unresolved_match.group(1) if unresolved_match else \"unknown\"\n",
    "        \n",
    "        # Extract available columns\n",
    "        available_match = re.search(r\"given input columns: \\[(.*?)\\]\", error_message)\n",
    "        available_columns = available_match.group(1).split(\", \") if available_match else []\n",
    "        \n",
    "        result[\"description\"] = f\"Column '{unresolved_column}' not found in DataFrame\"\n",
    "        result[\"details\"] = {\n",
    "            \"unresolved_column\": unresolved_column,\n",
    "            \"available_columns\": available_columns\n",
    "        }\n",
    "    \n",
    "    # Extract table name from \"Table or view not found: `table_name`\"\n",
    "    elif \"Table or view not found\" in error_message:\n",
    "        result[\"error_type\"] = \"MissingTableError\"\n",
    "        result[\"short_name\"] = \"TableNotFound\"\n",
    "        \n",
    "        # Extract the missing table name\n",
    "        table_match = re.search(r\"Table or view not found: (?:`)?([^'`]+)(?:`)?\", error_message)\n",
    "        table_name = table_match.group(1) if table_match else \"unknown\"\n",
    "        \n",
    "        result[\"description\"] = f\"Table or view '{table_name}' not found\"\n",
    "        result[\"details\"] = {\n",
    "            \"missing_table\": table_name\n",
    "        }\n",
    "    \n",
    "    # Handle ambiguous column references\n",
    "    elif \"Ambiguous column reference\" in error_message or \"Ambiguous column name\" in error_message:\n",
    "        result[\"error_type\"] = \"AmbiguousColumnError\"\n",
    "        result[\"short_name\"] = \"AmbiguousColumn\"\n",
    "        \n",
    "        # Extract the ambiguous column name\n",
    "        col_match = re.search(r\"[Aa]mbiguous column (?:reference|name)(?:: | ')([^']+)\", error_message)\n",
    "        col_name = col_match.group(1) if col_match else \"unknown\"\n",
    "        \n",
    "        result[\"description\"] = f\"Column '{col_name}' is ambiguous (exists in multiple tables/sources)\"\n",
    "        result[\"details\"] = {\n",
    "            \"ambiguous_column\": col_name\n",
    "        }\n",
    "    \n",
    "    # Handle file/path not found errors\n",
    "    elif \"Path does not exist\" in error_message:\n",
    "        result[\"error_type\"] = \"PathNotFoundError\"\n",
    "        result[\"short_name\"] = \"PathNotFound\"\n",
    "        \n",
    "        # Extract the missing path\n",
    "        path_match = re.search(r\"Path does not exist: ([^']+)\", error_message)\n",
    "        path = path_match.group(1) if path_match else \"unknown\"\n",
    "        \n",
    "        result[\"description\"] = f\"Path '{path}' does not exist\"\n",
    "        result[\"details\"] = {\n",
    "            \"missing_path\": path\n",
    "        }\n",
    "    \n",
    "    # Handle data type mismatch errors\n",
    "    elif \"data type mismatch\" in error_message.lower():\n",
    "        result[\"error_type\"] = \"DataTypeMismatchError\"\n",
    "        result[\"short_name\"] = \"DataTypeMismatch\"\n",
    "        result[\"description\"] = \"Data type mismatch in operation\"\n",
    "        \n",
    "        # Try to extract more details if available\n",
    "        type_match = re.search(r\"CAST\\(([^)]+) AS ([^)]+)\\)\", error_message)\n",
    "        if type_match:\n",
    "            result[\"details\"] = {\n",
    "                \"from_type\": type_match.group(1),\n",
    "                \"to_type\": type_match.group(2)\n",
    "            }\n",
    "    \n",
    "    # Handle schema mismatch errors\n",
    "    elif \"schema mismatch\" in error_message.lower():\n",
    "        result[\"error_type\"] = \"SchemaMismatchError\"\n",
    "        result[\"short_name\"] = \"SchemaMismatch\"\n",
    "        result[\"description\"] = \"Schema mismatch between operations\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81117e91",
   "metadata": {},
   "source": [
    "# Decorators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_try_except(func):\n",
    "    \"\"\"Decorator providing robust exception handling for class methods.\n",
    "\n",
    "    This decorator wraps a method and intercepts common exceptions that may occur during\n",
    "    data quality operations, including:\n",
    "        - KeyError: Raised when a dictionary key is missing.\n",
    "        - IndexError: Raised when a list or sequence index is out of bounds.\n",
    "        - AnalysisException: Raised by PySpark SQL for issues such as missing columns, tables, or schema mismatches.\n",
    "        - General Exception: Catches all other unexpected exceptions.\n",
    "\n",
    "    For each exception, the decorator:\n",
    "        - Logs the error message to self._logger if a logger is present.\n",
    "        - Prints the error message to standard output.\n",
    "        - Appends a structured error entry to self._row_errors (if available), including error type, method name, message, and details.\n",
    "\n",
    "    If an exception is caught, the method returns None by default.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The instance method to be wrapped.\n",
    "\n",
    "    Returns:\n",
    "        callable: The wrapped method with exception handling.\n",
    "\n",
    "    Example:\n",
    "        @add_try_except\n",
    "        def log_processed_file(self, source_data_df: DataFrame, success: bool) -> None:\n",
    "            # Method implementation...\n",
    "\n",
    "    Notes:\n",
    "        - This decorator assumes the wrapped method is an instance method with 'self' as its first argument.\n",
    "        - The decorator should be defined before the class and applied to any method where robust error handling is desired.\n",
    "        - If VERBOSE_LOGGING is enabled on the instance, additional details and stacktraces are logged for general exceptions and AnalysisException.\n",
    "        - The decorator is designed for use in data quality, validation, and ETL workflows where consistent error handling and logging are required.\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self, *args, **kwargs):\n",
    "        try:\n",
    "            return func(self, *args, **kwargs)\n",
    "        except KeyError as e:\n",
    "            error_msg = f\"KeyError in {func.__name__}: {str(e)}\"\n",
    "            if hasattr(self, 'logger') and self._logger:\n",
    "                self._logger.error(error_msg)\n",
    "            print(error_msg)\n",
    "            if hasattr(self, 'error_logs'):\n",
    "                self._row_errors.append({\n",
    "                    \"error_type\": \"KeyError\",\n",
    "                    \"method\": func.__name__,\n",
    "                    \"message\": str(e)\n",
    "                })\n",
    "            return None\n",
    "            \n",
    "        except IndexError as e:\n",
    "            error_msg = f\"IndexError in {func.__name__}: {str(e)}\"\n",
    "            if hasattr(self, 'logger') and self._logger:\n",
    "                self._logger.error(error_msg)\n",
    "            print(error_msg)\n",
    "            if hasattr(self, 'error_logs'):\n",
    "                self._row_errors.append({\n",
    "                    \"error_type\": \"IndexError\",\n",
    "                    \"method\": func.__name__,\n",
    "                    \"message\": str(e)\n",
    "                })\n",
    "            return None\n",
    "            \n",
    "        except AnalysisException as e:\n",
    "            # Use the analysis function for PySpark errors\n",
    "            error_info = analyze_analysis_exception(e)\n",
    "            error_msg = f\"AnalysisException in {func.__name__}: {error_info['description']}\"\n",
    "            \n",
    "            if hasattr(self, 'logger') and self._logger:\n",
    "                self._logger.error(error_msg)\n",
    "                if hasattr(self, 'VERBOSE_LOGGING') and self._verbose_logging:\n",
    "                    self._logger.error(f\"Details: {error_info['details']}\")\n",
    "            \n",
    "            print(error_msg)\n",
    "            \n",
    "            if hasattr(self, 'error_logs'):\n",
    "                self._row_errors.append({\n",
    "                    \"error_type\": error_info['error_type'],\n",
    "                    \"method\": func.__name__,\n",
    "                    \"message\": error_info['description'],\n",
    "                    \"details\": error_info['details'] if 'details' in error_info else {}\n",
    "                })\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Exception in {func.__name__}: {str(e)}\"\n",
    "            if hasattr(self, 'logger') and self._logger:\n",
    "                self._logger.error(error_msg)\n",
    "                if hasattr(self, 'VERBOSE_LOGGING') and self._verbose_logging:\n",
    "                    self._logger.error(traceback.format_exc())\n",
    "            print(error_msg)\n",
    "            if hasattr(self, 'error_logs'):\n",
    "                self._row_errors.append({\n",
    "                    \"error_type\": \"GeneralException\",\n",
    "                    \"method\": func.__name__,\n",
    "                    \"message\": str(e),\n",
    "                    \"stacktrace\": traceback.format_exc() if hasattr(self, 'VERBOSE_LOGGING') and self._verbose_logging else None\n",
    "                })\n",
    "            return None\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_method_call(stage: str):\n",
    "    \"\"\"\n",
    "    Decorator to log method calls in a class.\n",
    "\n",
    "    Each time the decorated method is called, a dictionary is appended to the instance's `method_log` list.\n",
    "    The log entry contains:\n",
    "        - \"stage\": A string indicating the processing stage (e.g., \"file_validation\", \"row_processing\").\n",
    "        - \"method\": The name of the method being called.\n",
    "        - \"parameters\": A dictionary of the positional and keyword arguments supplied to the method.\n",
    "        - \"status\": \"success\" if the method completes without exception, \"failure\" otherwise.\n",
    "\n",
    "    Args:\n",
    "        stage (str): The processing stage to associate with this method.\n",
    "\n",
    "    Usage:\n",
    "        @log_method_call(\"file_validation\")\n",
    "        def file_validation(self, ...):\n",
    "            ...\n",
    "\n",
    "    The class using this decorator should have a `method_log` attribute (a list) to store log entries.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            log_entry = {\n",
    "                \"stage\": stage,\n",
    "                \"method\": func.__name__,\n",
    "                \"parameters\": {\"args\": args, \"kwargs\": kwargs},\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            try:\n",
    "                result = func(self, *args, **kwargs)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                log_entry[\"status\"] = \"failure\"\n",
    "                raise\n",
    "            finally:\n",
    "                self._method_calls.append(log_entry)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd2fa8",
   "metadata": {},
   "source": [
    "# Data Quality Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityLibrary:\n",
    "    \"\"\"Provides a suite of methods for validating data quality, transforming data and logging errors in Spark-based data pipelines.\n",
    "\n",
    "    Features:\n",
    "    - File and field-level validation using configurable rules.\n",
    "    - Logging of processed files and failed rows for traceability.\n",
    "    - Error handling and logging with support for verbose output.\n",
    "    - Conversion of logs to Spark DataFrames for further analysis or persistence.\n",
    "    - Writing error logs to Delta tables in a Lakehouse architecture.\n",
    "    - Decorator-based robust error handling for all methods.\n",
    "\n",
    "    Args:\n",
    "        spark: The active SparkSession.\n",
    "        run_id (str): Unique identifier for the current pipeline run.\n",
    "        trigger_time (str): Timestamp or identifier for the pipeline trigger event.\n",
    "        verbose_logging (bool, optional): Enable detailed logging. Default is False.\n",
    "        logger (optional): Logger instance for custom logging. If None, uses default logging.\n",
    "\n",
    "    Example usage:\n",
    "        dq = DataQualityLibrary(spark, run_id=\"123\", trigger_time=\"2025-09-05\", verbose_logging=True, logger=my_logger)\n",
    "        dq.file_validation(df, file_rules, \"myfile.csv\")\n",
    "        dq.field_validation(df, field_rules, file_rules, \"table_name\")\n",
    "    \"\"\"\n",
    "    def __init__(self, spark, run_id: str, trigger_time: str, verbose_logging=False, logger=None) -> None:\n",
    "        self._row_errors = []\n",
    "        self._method_calls = []\n",
    "        self._verbose_logging = verbose_logging\n",
    "        self._logger = logger\n",
    "        self._spark = spark\n",
    "        self._run_id = run_id\n",
    "        self._trigger_time = trigger_time\n",
    "        self._processed_files = []\n",
    "        self._initial_rowcount = None\n",
    "    \n",
    "    @add_try_except\n",
    "    def _get_lookup_table(self, table_name: str) -> DataFrame:\n",
    "        \"\"\"Use a Fabric lakehouse shortcut to retrieve a lookup table as a Spark DataFrame.\"\"\"\n",
    "        lookup_df = self._spark.table(table_name)\n",
    "        \n",
    "        return lookup_df\n",
    "\n",
    "    @add_try_except\n",
    "    def toggle_verbose_logging(self) -> None:\n",
    "        \"\"\"Toggles the verbose logging flag.\"\"\"\n",
    "        self._verbose_logging = not self._verbose_logging\n",
    "\n",
    "    @add_try_except\n",
    "    def processed_files_to_df(self) -> DataFrame:\n",
    "        \"\"\"Converts the processed_files list to a Spark DataFrame.\"\"\"\n",
    "        return self._spark.createDataFrame(self._processed_files)\n",
    "\n",
    "    @add_try_except\n",
    "    def reset_error_logs(self):\n",
    "        \"\"\"Resets the error_logs property to an empty list.\"\"\"\n",
    "        self._row_errors = []\n",
    "\n",
    "    @add_try_except\n",
    "    def row_errors_to_df(self) -> DataFrame:\n",
    "        \"\"\"Converts the error_logs list to a Spark DataFrame.\"\"\"\n",
    "        return self._spark.createDataFrame(self._row_errors)\n",
    "    \n",
    "    @add_try_except\n",
    "    def method_calls_to_df(self) -> DataFrame:\n",
    "        \"\"\"Converts the method_calls list to a Spark DataFrame.\"\"\"\n",
    "        return self._spark.createDataFrame(self._method_calls)\n",
    "\n",
    "    @add_try_except\n",
    "    def _log_failed_rows(self, data_df: DataFrame, validation: str, rule_name: str, validation_stage: str) -> DataFrame:\n",
    "        \"\"\"Logs rows which have failed validation.\"\"\"\n",
    "        # Get rows which contain an error message\n",
    "        failed_rows_df = data_df.where(F.col(\"error_message\").isNotNull())\n",
    "        # Create a log entry for each row which failed validation\n",
    "        for row in failed_rows_df.collect():\n",
    "            log = {\n",
    "                \"datasource\": row.datasource,\n",
    "                \"entity_name\": row.entity_name,\n",
    "                \"row_id\": row.row_id,\n",
    "                \"validation\": validation,\n",
    "                \"rule_name\":rule_name,\n",
    "                \"error_message\": row.error_message,\n",
    "                \"stage\": validation_stage\n",
    "            }\n",
    "            self._row_errors.append(log)\n",
    "            if self._verbose_logging:\n",
    "                self._logger.warning(\"datasource '%s' entity '%s' row '%s' failed with error in %s: %s. Error: %s\", row.datasource, row.entity_name, row.row_id, validation, rule_name)\n",
    "        # Drop error message column\n",
    "        data_df = data_df.drop(\"error_message\")\n",
    "\n",
    "        return data_df\n",
    "    \n",
    "    # File-level validation functions\n",
    "    @add_try_except\n",
    "    def file_validation(self, source_data_df: DataFrame, metadata_rules: dict, source_filepath: str) -> bool:\n",
    "        \"\"\"Function to validate the columns of the source file.\n",
    "        The function validates that all mandatory columns are present, and that no extraneous columns are\n",
    "        present in the source file.\n",
    "        Args:\n",
    "            source_data_df (DataFrame): DataFrame containing data from the source file.\n",
    "            metadata_rules (dict): Dictionary containing the metadata from file_<datasource>.yaml.\n",
    "            source_filepath (str): The name of the file being validated.\n",
    "        Returns:\n",
    "            Bool: Validation results.\n",
    "            source_data_df (DataFrame): DataFrame potentially modified to add missing optional columns or drop ignored columns.\n",
    "        \"\"\"\n",
    "        # Lists for column comparison.\n",
    "        all_metadata_columns = []\n",
    "        all_df_columns = source_data_df.columns\n",
    "        # Read in columns metadata\n",
    "        mandatory_columns = metadata_rules.get(\"mandatory_columns\")\n",
    "        optional_columns = metadata_rules.get(\"optional_columns\", [])\n",
    "        ignore_columns = metadata_rules.get(\"ignore_columns\", [])\n",
    "        for item in mandatory_columns:\n",
    "            column_name = item[\"name\"]\n",
    "            all_metadata_columns.append(column_name)\n",
    "            # Check that the mandatory column exists in source\n",
    "            if not column_name in all_df_columns:\n",
    "                if self._verbose_logging:\n",
    "                    self._logger.warning(f\"File {source_filepath} does not contain the mandatory column: {column_name}\")\n",
    "                return False, source_data_df\n",
    "        # Add optional columns to all_metadata_columns\n",
    "        for item in optional_columns:\n",
    "            column_name = item[\"name\"]\n",
    "            all_metadata_columns.append(column_name)\n",
    "            # Add the optional column if it doesn't exist in source df\n",
    "            if not column_name in source_data_df.columns:\n",
    "                source_data_df = source_data_df.withColumn(column_name, F.lit(None))\n",
    "        # Add ignore columns all_metadata_columns\n",
    "        for item in ignore_columns:\n",
    "            ignore_col = item[\"name\"]\n",
    "            if ignore_col in source_data_df.columns:\n",
    "                source_data_df = source_data_df.drop(ignore_col)\n",
    "            all_metadata_columns.append(ignore_col)\n",
    "        # Check that no extraneous columns are in source data\n",
    "        is_all_columns = set(all_df_columns).issubset(set(all_metadata_columns))\n",
    "        if not is_all_columns:\n",
    "            if self._verbose_logging:\n",
    "                self._logger.warning(f\"File {source_filepath} contains unexpected columns\")\n",
    "            return False, source_data_df\n",
    "        return True, source_data_df\n",
    "\n",
    "    # Field-level validation functions\n",
    "    @add_try_except\n",
    "    @log_method_call(\"field_validation\")\n",
    "    def field_validation(self, source_data_df: DataFrame, metadata_rules: dict) -> None:\n",
    "        \"\"\"Main function to do the field validations for a given entity.\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Source dataframe \n",
    "            metadata_rules (dict): Dictionary containing rules to do the validation. This dictionary is generated from the file field_validation_datasource.yaml\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        rules = metadata_rules.get(\"rules\")\n",
    "\n",
    "        for rule in rules:\n",
    "            processing_type = rule.get(\"type\")\n",
    "            rule_name = rule.get(\"name\")\n",
    "            custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "\n",
    "            match processing_type:\n",
    "                case \"expect_column_values_to_not_be_null\":\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "                    conditions = rule.get(\"conditions\", None)\n",
    "\n",
    "                    self._expect_column_values_to_not_be_null(null_check_df=source_data_df,\n",
    "                                                              columns=columns_to_validate,\n",
    "                                                              custom_error_message=custom_error_code,\n",
    "                                                              rule_name=rule_name,\n",
    "                                                              conditions=conditions)\n",
    "                            \n",
    "                case \"expect_column_distinct_values_to_be_in_set\":\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "                    values = rule.get(\"values\")\n",
    "\n",
    "                    self._expect_column_values_in_set(source_data_df=source_data_df,\n",
    "                                                      columns=columns_to_validate,\n",
    "                                                      custom_error_message=custom_error_code,\n",
    "                                                      value_set=values,\n",
    "                                                      rule_name=rule_name)     \n",
    "\n",
    "                case \"expect_column_values_to_be_null\":\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "\n",
    "                    self._expect_column_values_to_be_null(source_data_df=source_data_df,\n",
    "                                                          columns=columns_to_validate,\n",
    "                                                          custom_error_message=custom_error_code,\n",
    "                                                          rule_name=rule_name)\n",
    "                    \n",
    "                case \"expect_at_least_one_non_null_column\":\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "                    conditions = rule.get(\"conditions\", None)\n",
    "\n",
    "                    self._expect_at_least_one_non_null_column(one_non_null_df=source_data_df,\n",
    "                                                              columns=columns_to_validate,\n",
    "                                                              custom_error_message=custom_error_code,\n",
    "                                                              rule_name=rule_name,\n",
    "                                                              conditions=conditions)\n",
    "                    \n",
    "                case \"expect_relative_date\":\n",
    "                    time_type = rule.get(\"time\")\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "\n",
    "                    self._expect_relative_date(source_data_df=source_data_df,\n",
    "                                               columns=columns_to_validate,\n",
    "                                               custom_error_message=custom_error_code,\n",
    "                                               time_check_type=time_type,\n",
    "                                               rule_name=rule_name)\n",
    "                    \n",
    "                case \"expect_column_values_to_be_between\":\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "                    min_value, max_value = rule.get(\"min_value\", None), rule.get(\"max_value\", None)\n",
    "                    strict_min, strict_max = rule.get(\"strict_min\", False), rule.get(\"strict_max\", False)\n",
    "\n",
    "                    self._expect_column_values_between(source_data_df=source_data_df,\n",
    "                                                       columns=columns_to_validate,\n",
    "                                                       min_value=min_value,\n",
    "                                                       max_value=max_value,\n",
    "                                                       strict_min=strict_min,\n",
    "                                                       strict_max=strict_max,\n",
    "                                                       custom_error_message=custom_error_code,\n",
    "                                                       rule_name=rule_name)\n",
    "                    \n",
    "                case \"expect_format\":\n",
    "                    columns_to_validate = rule.get(\"columns\")\n",
    "                    format = rule.get(\"format\", None)\n",
    "                    length = rule.get(\"length\", None)\n",
    "\n",
    "                    self._expect_format(source_data_df=source_data_df,\n",
    "                                        columns=columns_to_validate,\n",
    "                                        format=format,\n",
    "                                        length=length,\n",
    "                                        custom_error_message=custom_error_code,\n",
    "                                        rule_name=rule_name)\n",
    "                \n",
    "                case _:\n",
    "                    if self._verbose_logging:\n",
    "                        self._logger.warning(f\"Unknown processing type: {processing_type} in rule: {rule_name}\")\n",
    "        \n",
    "        length_validations = [item for item in metadata_rules[\"mandatory_columns\"] if \"max_length\" in item]\n",
    "        self._column_length_checks(source_data_df=source_data_df,\n",
    "                                   length_check_validations=length_validations,\n",
    "                                   rule_name=\"field_length\")\n",
    "\n",
    "        return\n",
    "\n",
    "    @add_try_except\n",
    "    def _expect_column_values_between(self, source_data_df: DataFrame, columns: list, min_value: int, max_value: int, strict_min: bool, strict_max: bool, custom_error_message: str, rule_name:str) -> None:\n",
    "        \"\"\"Checks column values are between two supplied values.\n",
    "        Checks a Spark DataFrame to validate that values in the specified column name are between a minimum and maximum value.\n",
    "        min_value and max_value are both inclusive unless strict_min or strict_max are set to True. If min_value is None, then max_value\n",
    "        is treated as an upper bound, and there is no minimum value checked. If max_value is None, then min_value is treated as a lower bound, and\n",
    "        there is no maximum value checked. By default, null values are not considered in computation and so will be handled by separate checks.\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark DataFrame to validate\n",
    "            columns (list): Name of the column to validate.\n",
    "            min_value (int): Minimum allowed column value.\n",
    "            max_value (int): Maximum allowed column value.\n",
    "            strict_min (bool): Only allow > comparison as opposed to >=.\n",
    "            strict_max (bool): Only allow < comparison as opposed to <=.\n",
    "            custom_error_message (str): Custom error message if provided in the metadata.\n",
    "            rule_name (str): Name of the metadata rule that called this function.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        validation = \"Expect Between\"\n",
    "        for column in columns:\n",
    "            # Cast column to int for comparison\n",
    "            source_data_df = source_data_df.withColumn(f\"{column}_int\", F.col(column).cast(\"int\"))\n",
    "\n",
    "            # Check for non-integer values and log error if found\n",
    "            source_data_df = source_data_df.withColumn(\n",
    "                \"error_message\",\n",
    "                F.when(\n",
    "                    (F.col(column).isNotNull()) & (F.col(f\"{column}_int\").isNull()),\n",
    "                    F.concat(\n",
    "                        F.lit(f\"Column '{column}' with value '\"),\n",
    "                        F.col(column),\n",
    "                        F.lit(\"' must be an integer\")\n",
    "                    )\n",
    "                ).otherwise(F.lit(None))\n",
    "            )\n",
    "\n",
    "            # Replace original column with casted integer column for further checks\n",
    "            source_data_df.drop(f\"{column}_int\")\n",
    "\n",
    "            # Both min and max provided â†’ BETWEEN check\n",
    "            if min_value is not None and max_value is not None:\n",
    "                message = custom_error_message if custom_error_message else f\"{column} must be between {min_value} and {max_value}\"\n",
    "                if strict_min and strict_max:\n",
    "                    condition = (F.col(column) <= min_value) | (F.col(column) >= max_value)\n",
    "                elif strict_min:\n",
    "                    condition = (F.col(column) <= min_value) | (F.col(column) > max_value)\n",
    "                elif strict_max:\n",
    "                    condition = (F.col(column) < min_value) | (F.col(column) >= max_value)\n",
    "                else:\n",
    "                    condition = (F.col(column) < min_value) | (F.col(column) > max_value)\n",
    "                \n",
    "                error_message = F.when(\n",
    "                    condition & F.col(\"error_message\").isNull(),\n",
    "                    message\n",
    "                ).otherwise(F.col(\"error_message\"))            \n",
    "            # Only min provided\n",
    "            elif min_value is not None:\n",
    "                message = custom_error_message if custom_error_message else f\"{column} must be greater than {min_value}.\"\n",
    "                if strict_min:\n",
    "                    condition = F.col(column) <= min_value\n",
    "                else:\n",
    "                    condition = F.col(column) < min_value\n",
    "\n",
    "                error_message = F.when(\n",
    "                    condition & F.col(\"error_message\").isNull(),\n",
    "                    message\n",
    "                ).otherwise(F.col(\"error_message\"))\n",
    "            # Only max provided\n",
    "            elif max_value is not None:\n",
    "                message = custom_error_message if custom_error_message else f\"{column} must be less than {max_value}.\"\n",
    "                if strict_max:\n",
    "                    condition = F.col(column) >= max_value\n",
    "                else:\n",
    "                    condition = F.col(column) > max_value\n",
    "                error_message = F.when(\n",
    "                    condition & F.col(\"error_message\").isNull(),\n",
    "                    message\n",
    "                ).otherwise(F.col(\"error_message\"))\n",
    "\n",
    "            self._log_failed_rows(data_df=source_data_df,\n",
    "                                  validation=validation,\n",
    "                                  rule_name=rule_name,\n",
    "                                  validation_stage=\"field_validation\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    @add_try_except\n",
    "    def _expect_column_values_in_set(self, source_data_df: DataFrame, columns: list, custom_error_message: str, value_set: list, rule_name:str) -> None:\n",
    "        \"\"\"Function to validate if a value in a column is in a given value_set. The validation is run one column at a time and append the \n",
    "        failed rows to a global error list.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark DataFrame to validate.\n",
    "            columns (list): List of column(s) in the DataFrame to be validated.\n",
    "            custom_error_message (str): Custom error message to use.\n",
    "            value_set (list): List of the expected value that the given column value should be in.\n",
    "            rule_name (str): Name of the metadata rule that called this function.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        validation=\"Values in Set\"\n",
    "\n",
    "        for column in columns:        \n",
    "            error_message = custom_error_message if custom_error_message else f\"Column {column} contains a value that is not in the list: {value_set}\"\n",
    "\n",
    "            # Get rows which do not have a value in the value_set\n",
    "            errors_df = source_data_df.where(~F.col(column).isin(value_set))\n",
    "\n",
    "            # Add error message column\n",
    "            errors_df = errors_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "\n",
    "            # Log the failed rows\n",
    "            self._log_failed_rows(errors_df, validation, rule_name, \"field_validation\")        \n",
    "        \n",
    "        return\n",
    "    \n",
    "    @add_try_except\n",
    "    def _expect_column_values_to_be_null(self, null_check_df: DataFrame, columns: list, custom_error_message: str, rule_name: str, conditions: list = None):\n",
    "        \"\"\"Validates if a column is not null.\n",
    "        Validate if a column is not NULL. If any column in the columns list is Null, flag it as an \n",
    "        error and add the custom_error_message. If supplied, the optional conditions parameter is \n",
    "        used to filter the dataframe before validation, ensuring the check is only applied to a \n",
    "        specific subset of data.\n",
    "\n",
    "        Args:\n",
    "            null_check_df (DataFrame): Spark DataFrame to validate\n",
    "            columns (list[str]): List of column(s) in the DataFrame to be validated\n",
    "            custom_error_message (str): Custom error message to use.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            conditions (list[dict]): List of dictionaries which define conditions on which to apply the check.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        validation = \"Expect Not Null\"\n",
    "\n",
    "        if conditions:\n",
    "            for condition in conditions:\n",
    "                column = condition.get(\"column\")\n",
    "                comparison = condition.get(\"comparison\")\n",
    "                value = condition.get(\"value\")\n",
    "\n",
    "                # Apply conditions to filter the dataframe                \n",
    "                if comparison == \"eq\":\n",
    "                    null_check_df = null_check_df.where(F.col(column) == value)\n",
    "                elif comparison == \"ne\":\n",
    "                    null_check_df = null_check_df.where((F.col(column) != value) | (F.col(column).isNull()))\n",
    "        \n",
    "        for column in columns:\n",
    "            # Define error message\n",
    "            error_message = custom_error_message if custom_error_message else f\"Column {column} cannot be null but contains a null value\"        \n",
    "\n",
    "            # Get rows which have a null value in the column\n",
    "            errors_df = null_check_df.where(F.col(column).isNull())\n",
    "\n",
    "            # Add error message column\n",
    "            errors_df = errors_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "\n",
    "            # Log the failed rows\n",
    "            self._log_failed_rows(data_df=errors_df,\n",
    "                                  validation=validation,\n",
    "                                  rule_name=rule_name,\n",
    "                                  validation_stage=\"field_validation\")        \n",
    "\n",
    "        return\n",
    "    \n",
    "    @add_try_except\n",
    "    def _expect_column_values_to_not_be_null(self, null_check_df: DataFrame, columns: list[str], custom_error_message: str, rule_name: str, conditions: list[dict]) -> None:\n",
    "        \"\"\"Validates if a column is not null.\n",
    "        Validate if a column is not NULL. If any column in the columns list is Null, flag it as an \n",
    "        error and add the custom_error_message. If supplied, the optional conditions parameter is \n",
    "        used to filter the dataframe before validation, ensuring the check is only applied to a \n",
    "        specific subset of data.\n",
    "\n",
    "        Args:\n",
    "            null_check_df (DataFrame): Spark DataFrame to validate\n",
    "            columns (list[str]): List of column(s) in the DataFrame to be validated\n",
    "            custom_error_message (str): Custom error message to use.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            conditions (list[dict]): List of dictionaries which define conditions on which to apply the check.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        validation = \"Expect Not Null\"\n",
    "\n",
    "        if conditions:\n",
    "            for condition in conditions:\n",
    "                column = condition.get(\"column\")\n",
    "                comparison = condition.get(\"comparison\")\n",
    "                value = condition.get(\"value\")\n",
    "                \n",
    "                if comparison == \"eq\":\n",
    "                    null_check_df = null_check_df.where(F.col(column) == value)\n",
    "                elif comparison == \"ne\":\n",
    "                    null_check_df = null_check_df.where((F.col(column) != value) | (F.col(column).isNull()))\n",
    "        \n",
    "        for column in columns:\n",
    "            error_message = custom_error_message if custom_error_message else f\"Column {column} cannot be null but contains a null value\"        \n",
    "\n",
    "            errors_df = null_check_df.where(F.col(column).isNull())\n",
    "            errors_df = errors_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "            self._log_failed_rows(data_df=errors_df,\n",
    "                                  validation=validation,\n",
    "                                  rule_name=rule_name,\n",
    "                                  validation_stage=\"field_validation\")\n",
    "\n",
    "        return\n",
    "\n",
    "    @add_try_except\n",
    "    def _expect_at_least_one_non_null_column(self, one_non_null_df: DataFrame, columns: list, custom_error_message: str, rule_name:str, conditions: list[dict] = None) -> None:\n",
    "        \"\"\"Validates if ALL columns in the columns list are null.\n",
    "        At least one of the columns in the list MUST have a value. If supplied, the \n",
    "        optional conditions parameter is used to filter the dataframe before validation, \n",
    "        ensuring the check is only applied to a specific subset of data.\n",
    "\n",
    "        Args:\n",
    "            one_non_null_df (DataFrame) : Dataframe to validate.\n",
    "            columns (list)              : List of columns to check for Nulls.\n",
    "            custom_error_message (str)  : Custom error message. This string has 'column_placeholder' in it which \n",
    "                                            will be replaced by the current column that is being validated.\n",
    "            rule_name (str)             : Name of the metadata rule that called this function.\n",
    "            conditions (list[dict])     : List of dictionaries which define conditions on which to apply the check.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        validation=\"Expect One Non Null\"\n",
    "\n",
    "        if conditions:\n",
    "            for condition in conditions:\n",
    "                column = condition.get(\"column\")\n",
    "                comparison = condition.get(\"comparison\")\n",
    "                value = condition.get(\"value\")\n",
    "                \n",
    "                if comparison == \"eq\":\n",
    "                    one_non_null_df = one_non_null_df.where(F.lower(F.col(column)) == value.lower())\n",
    "                elif comparison == \"ne\":\n",
    "                    one_non_null_df = one_non_null_df.where((F.lower(F.col(column)) != value.lower()) | (F.col(column).isNull()))\n",
    "        \n",
    "        error_message = custom_error_message if custom_error_message else f\"The columns {columns} must contain at least one non null value. Populate at least one of these columns and resubmit\"\n",
    "        \n",
    "        errored_df = (\n",
    "            one_non_null_df\n",
    "            .withColumn(\"Result\", F.when(F.coalesce(*columns).isNull(), \"Error\").otherwise(\"Okay\"))\n",
    "            .where(\"Result == 'Error'\")\n",
    "        )\n",
    "\n",
    "        errored_df = errored_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "\n",
    "        self._log_failed_rows(data_df=errored_df,\n",
    "                              validation=validation,\n",
    "                              rule_name=rule_name,\n",
    "                              validation_stage=\"field_validation\")\n",
    "        \n",
    "        return\n",
    "\n",
    "    @add_try_except    \n",
    "    def _expect_relative_date(self, source_data_df: DataFrame, columns: list, custom_error_message: str, time_check_type: str, rule_name:str) -> None:\n",
    "        \"\"\"Function to validate if a given date is today or earlier, or that a date is in the future.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Dataframe to validate.\n",
    "            columns (list): Columns to validate.\n",
    "            custom_error_message (str): Custom error message. This string has 'column_placeholder' in it which \n",
    "                                        will be replaced by the current column that is being validated.\n",
    "            time_check_type (str): If time is today_or_earlier then that means all values in columns <= today. \n",
    "                                   If time is today_or_later that means all values in columns >= today. If the column is Null, ignore it.\n",
    "            rule_name (str): Name of the metadata rule which called this function.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        validation = \"Expect Relative Date\"\n",
    "        for column in columns:\n",
    "            error_message = custom_error_message if custom_error_message else f\"The column {column} must contain a date that is {time_check_type}\"\n",
    "\n",
    "            if time_check_type == \"today_or_earlier\":\n",
    "                errored_df = (\n",
    "                    source_data_df\n",
    "                    .withColumn(\"Result\", F.when(\n",
    "                                            F.col(column).isNull(), \"Okay\").otherwise(\n",
    "                                                F.when(\n",
    "                                                    F.to_date(column, \"dd/MM/yyyy\") <= F.current_date(), \"Okay\"\n",
    "                                                ).otherwise(\"Error\")\n",
    "                                            ))\n",
    "                    .where(\"Result == 'Error'\")\n",
    "                )\n",
    "            elif time_check_type == \"today_or_later\":\n",
    "                errored_df = (\n",
    "                    source_data_df\n",
    "                    .withColumn(\"Result\", F.when(\n",
    "                                            F.col(column).isNull(), \"Okay\").otherwise(\n",
    "                                                F.when(\n",
    "                                                    F.to_date(column, \"dd/MM/yyyy\") >= F.current_date(), \"Okay\"\n",
    "                                                ).otherwise(\"Error\")\n",
    "                                            ))\n",
    "                    .where(\"Result == 'Error'\")\n",
    "                )\n",
    "                        \n",
    "            errored_df = errored_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "            self._log_failed_rows(data_df=errored_df,\n",
    "                                  validation=validation,\n",
    "                                  rule_name=rule_name,\n",
    "                                  validation_stage=\"field_validation\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    @add_try_except\n",
    "    def _expect_format(self, source_data_df: DataFrame, columns: list, format: str, length: int, rule_name:str, custom_error_message: str) -> None:\n",
    "            \"\"\"Function to validate the format of a given column.\n",
    "            At the moment only possible format check is length of strings.\n",
    "\n",
    "            Args:\n",
    "                source_data_df (DataFrame): Dataframe to validate.\n",
    "                columns (list): Columns to validate.\n",
    "                custom_error_message (str): Custom error message. This string has 'column_placeholder' in it which \n",
    "                                            will be replaced by the current column that is being validated.\n",
    "                format (str): Type of format we are validating. Only 'string' is accepted for now.\n",
    "                length (int): Expected length of the string.\n",
    "                rule_name (str): Name of the metadata rule that called this function\n",
    "\n",
    "            Returns:\n",
    "                None\n",
    "            \"\"\"\n",
    "            validation = \"Expect Format\"\n",
    "\n",
    "            for column in columns:\n",
    "                error_message = custom_error_message if custom_error_message else f\"The values in column {column} must be of length {length}.\"\n",
    "                \n",
    "                # errors are rows where value is not blank and value length is not expected length\n",
    "                errored_df = source_data_df.where((F.length(F.col(column)) != length) & (F.col(column) != \"\") & (F.col(column).isNotNull()))\n",
    "                errored_df = errored_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "                self._log_failed_rows(data_df=errored_df,\n",
    "                                      validation=validation,\n",
    "                                      rule_name=rule_name,\n",
    "                                      validation_stage=\"field_validation\")                \n",
    "            \n",
    "            return\n",
    "    \n",
    "    @add_try_except\n",
    "    def _column_length_checks(self, source_data_df: DataFrame, length_check_validations: list, rule_name:str) -> None:\n",
    "        \"\"\"Function to validate the length of a given column. If the column to validate does not exist in the dataframe,\n",
    "        then it is skipped and the column name is send to logger. If the column to validate is null, then it is skipped.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Dataframe to validate.\n",
    "            length_check_validations (list): List which contains column names and their corresponding max allowed length.\n",
    "            rule_name (str): Name of the metadata rule that called this function.\n",
    "            E.g:\n",
    "                [   \n",
    "                    {'name': 'GFRefNum', 'max_length': 200},\n",
    "                    {'name': 'FName', 'max_length': 200},\n",
    "                    {'name': 'SName', 'max_length': 200}\n",
    "                ]\n",
    "        \"\"\"\n",
    "        validation = \"Column Length\"\n",
    "        \n",
    "        for item in length_check_validations:\n",
    "            column = item.get(\"name\")\n",
    "            max_length = item.get(\"max_length\")\n",
    "\n",
    "            errored_df = (\n",
    "                source_data_df\n",
    "                .withColumn(\n",
    "                    \"Result\", F.when(\n",
    "                        F.col(column).isNull(), \"Okay\").otherwise(\n",
    "                            F.when(\n",
    "                                F.length(F.col(column)) <= max_length,\n",
    "                                \"Pass\"\n",
    "                            ).otherwise(\"Error\")\n",
    "                        )\n",
    "                    )\n",
    "                .where(\"Result == 'Error'\")\n",
    "            )\n",
    "\n",
    "            error_message = f\"The length of the column {column} exceeds max length of {max_length}\"\n",
    "            errored_df = errored_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "            self._log_failed_rows(data_df=errored_df,\n",
    "                                  validation=validation,\n",
    "                                  rule_name=rule_name,\n",
    "                                  validation_stage=\"field_validation\")        \n",
    "\n",
    "        return\n",
    "\n",
    "    # Row processing functions\n",
    "    @add_try_except\n",
    "    @log_method_call(\"row_processing\")\n",
    "    def row_processing(self, source_data_df: DataFrame, metadata_rules: dict) -> DataFrame:\n",
    "        \"\"\"Main function to do row processing for a given entity.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Source data frame.\n",
    "            metadata_rules (dict): Dictionary containing rules to do the row processing.\n",
    "\n",
    "        Returns:\n",
    "            Dataframe after all row processing functions have been called.\n",
    "        \"\"\"\n",
    "        \n",
    "        for rule in metadata_rules:\n",
    "            processing_type = rule.get(\"type\")\n",
    "            rule_name = rule.get(\"name\")\n",
    "\n",
    "            match processing_type:\n",
    "                case \"default\":\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    value = rule.get(\"value\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "\n",
    "                    source_data_df = self._set_default_column_value(data_df=source_data_df,\n",
    "                                                                    columns=columns,\n",
    "                                                                    default_value=value,\n",
    "                                                                    suffix=suffix,\n",
    "                                                                    rule_name=rule_name)\n",
    "\n",
    "                case \"copy_columns\":\n",
    "                    columns_to_copy = rule.get(\"copy_columns\")\n",
    "\n",
    "                    source_data_df = self._copy_columns(source_data_df=source_data_df,\n",
    "                                                        columns=columns_to_copy)\n",
    "\n",
    "                case \"concat\":\n",
    "                    columns_to_concat = rule.get(\"columns\")\n",
    "                    output = rule.get(\"output\")\n",
    "                    separator = rule.get(\"separator\", \"\")\n",
    "\n",
    "                    source_data_df = self._concat_column_values(source_data_df=source_data_df,\n",
    "                                                                columns=columns_to_concat,\n",
    "                                                                output_column=output,\n",
    "                                                                sep=separator,\n",
    "                                                                rule_name=rule_name)\n",
    "\n",
    "                case \"format_date\":\n",
    "                    conditions=rule.get(\"conditions\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "                    format_pattern = rule.get(\"format\")\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "    \n",
    "                    source_data_df = self._format_data(source_data_df=source_data_df,\n",
    "                                                       format_name=\"date\",\n",
    "                                                       format_pattern=format_pattern,\n",
    "                                                       columns=columns,\n",
    "                                                       suffix=suffix,\n",
    "                                                       conditions=conditions,\n",
    "                                                       error_message=custom_error_code,\n",
    "                                                       rule_name=rule_name)\n",
    "                    \n",
    "                case \"format_datetime\":                    \n",
    "                    conditions=rule.get(\"conditions\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "                    format_pattern = rule.get(\"format\")\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "\n",
    "                    source_data_df = self._format_data(source_data_df=source_data_df,\n",
    "                                                       format_name=\"datetime\",\n",
    "                                                       format_pattern=format_pattern,\n",
    "                                                       columns=columns,\n",
    "                                                       suffix=suffix,\n",
    "                                                       conditions=conditions,\n",
    "                                                       error_message=custom_error_code,\n",
    "                                                       rule_name=rule_name)\n",
    "\n",
    "                case \"format_post_code\":\n",
    "                    conditions=rule.get(\"conditions\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "                    format_pattern = None\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "    \n",
    "                    source_data_df = self._format_data(source_data_df=source_data_df,\n",
    "                                                       format_name=\"postcode\",\n",
    "                                                       format_pattern=format_pattern,\n",
    "                                                       columns=columns,\n",
    "                                                       suffix=suffix,\n",
    "                                                       conditions=conditions,\n",
    "                                                       error_message=custom_error_code,\n",
    "                                                       rule_name=rule_name)\n",
    "\n",
    "                case \"format_email_address\":\n",
    "                    conditions=rule.get(\"conditions\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "                    format_pattern = None\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "\n",
    "                    source_data_df = self._format_data(source_data_df=source_data_df,\n",
    "                                                       format_name=\"email\",\n",
    "                                                       format_pattern=format_pattern,\n",
    "                                                       columns=columns,\n",
    "                                                       suffix=suffix,\n",
    "                                                       conditions=conditions,\n",
    "                                                       error_message=custom_error_code,\n",
    "                                                       rule_name=rule_name)\n",
    "\n",
    "                case \"format_phone_number\":\n",
    "                    conditions = rule.get(\"conditions\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "                    format_pattern = None\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "    \n",
    "                    source_data_df = self._format_data(source_data_df=source_data_df,\n",
    "                                                       format_name=\"phone\",\n",
    "                                                       format_pattern=format_pattern,\n",
    "                                                       columns=columns,\n",
    "                                                       suffix=suffix,\n",
    "                                                       conditions=conditions,\n",
    "                                                       error_message=custom_error_code,\n",
    "                                                       rule_name=rule_name)\n",
    "\n",
    "                case \"date_part\":\n",
    "                    column = rule.get(\"column\")\n",
    "                    date_part = rule.get(\"date_part\")\n",
    "                    output_column_name = rule.get(\"output\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "    \n",
    "                    source_data_df = self._add_date_part(source_data_df=source_data_df,\n",
    "                                                         date_column=column,\n",
    "                                                         date_part=date_part,\n",
    "                                                         output_column=output_column_name,\n",
    "                                                         error_message=custom_error_code,\n",
    "                                                         rule_name=rule_name)\n",
    "\n",
    "                case \"remove_characters\":\n",
    "                    column_to_clean = rule.get(\"column\")\n",
    "                    characters_to_remove = rule.get(\"characters\")\n",
    "                    output_column_name = rule.get(\"output\")\n",
    "    \n",
    "                    source_data_df = self._remove_characters(data_df=source_data_df,\n",
    "                                                             characters=characters_to_remove,\n",
    "                                                             column_name=column_to_clean,\n",
    "                                                             output_column=output_column_name,\n",
    "                                                             rule_name=rule_name)\n",
    "\n",
    "                case \"backdate_four_tax_years\":\n",
    "                    column_to_backdate = rule.get(\"date\")\n",
    "                    output_column = rule.get(\"output\")\n",
    "    \n",
    "                    source_data_df = self._backdate_four_tax_years(data_df=source_data_df,\n",
    "                                                                   backdate_column=column_to_backdate,\n",
    "                                                                   output_column_name=output_column,\n",
    "                                                                   rule_name=rule_name)\n",
    "\n",
    "\n",
    "                case \"convert_values\":\n",
    "                    columns_to_convert = rule.get(\"columns\")\n",
    "                    replaces = rule.get(\"replaces\")\n",
    "                    suffix = rule.get(\"suffix\")\n",
    "                    custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "                    \n",
    "                    behaviour = rule.get(\"behaviour\", None)\n",
    "        \n",
    "                    source_data_df = self._convert_values(source_data_df=source_data_df,\n",
    "                                                          columns=columns_to_convert,\n",
    "                                                          replaces=replaces,\n",
    "                                                          suffix=suffix,\n",
    "                                                          error_message=custom_error_code,\n",
    "                                                          rule_name=rule_name,\n",
    "                                                          behaviour=behaviour)\n",
    "        \n",
    "                case \"create_field\":\n",
    "                    value = rule.get(\"value\", None)\n",
    "                    function = rule.get(\"function\", None)\n",
    "                    output = rule.get(\"output\")\n",
    "                    conditions = rule.get(\"conditions\", None)\n",
    "\n",
    "                    source_data_df = self._create_field(source_data_df=source_data_df,\n",
    "                                                        value=value,\n",
    "                                                        function=function,\n",
    "                                                        output=output,\n",
    "                                                        rule_name=rule_name,\n",
    "                                                        conditions=conditions) \n",
    "                \n",
    "                case \"first_non_null\":\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    output = rule.get(\"output\")\n",
    "                    source_data_df = self._first_non_null(source_data_df=source_data_df,\n",
    "                                                          columns=columns,\n",
    "                                                          output=output,\n",
    "                                                          rule_name=rule_name)\n",
    "                \n",
    "                case \"create_conditional_column\":\n",
    "                    if_thens = rule.get(\"if_thens\")\n",
    "                    else_value = rule.get(\"else\")\n",
    "                    output = rule.get(\"output\")\n",
    "\n",
    "                    source_data_df = self._create_conditional_column(source_data_df=source_data_df,\n",
    "                                                                     if_thens=if_thens,\n",
    "                                                                     else_value=else_value,\n",
    "                                                                     output=output,\n",
    "                                                                     rule_name=rule_name)\n",
    "                case _:\n",
    "                    if self._verbose_logging:\n",
    "                        self._logger.warning(f\"Unknown processing type: {processing_type} in rule: {rule_name}\")\n",
    "        \n",
    "\n",
    "        return source_data_df\n",
    "    \n",
    "        \n",
    "    def _format_data(self, source_data_df: DataFrame, format_name: str, format_pattern: str, columns: list[str], suffix:str, conditions: list[str], error_message: str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"Checks a data source meets the expected format.\n",
    "        The function checks the format_name parameter and performs transformation\n",
    "        and validation operations. If any rows in the supplied Dataframe do not\n",
    "        pass validation then the errors are logged.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark dataset containing the rows to validate.\n",
    "            format_name (str): The format to validate - must be one of email/phone/postcode/date.\n",
    "            format_pattern (str): An option expected format pattern (used for date).\n",
    "            columns (list[str]): A list of columns to apply this formatting to, e.g a list of email addresses.\n",
    "            suffix (str): The suffix to use for the transformed column name.\n",
    "            conditions (list[str]): Conditions under which to apply formatting, e.g where email address != \"anonymous\"\n",
    "            error_message (str): An optional custom error message which is only used if provided.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "        \n",
    "        Returns:\n",
    "            formatted_df (DataFrame): The transformed dataframe.\n",
    "        \"\"\"\n",
    "        formatted_df = source_data_df\n",
    "\n",
    "        match (format_name):\n",
    "            case \"email\":\n",
    "                validation = \"Format Email address\"\n",
    "\n",
    "                # Regular expression pattern for standard email address\n",
    "                email_regex = \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
    "\n",
    "                for column_name in columns:\n",
    "                    # Get error message if no custom metadata provided\n",
    "                    email_error_message = f\"{column_name} must be in a standard email address format\" if not error_message else error_message\n",
    "\n",
    "                    condition_expr = None\n",
    "                    if conditions:\n",
    "                        for condition in conditions:\n",
    "                            column = condition[\"column\"]\n",
    "                            comparison = condition[\"comparison\"]\n",
    "                            value = condition['value']\n",
    "\n",
    "                            if comparison == \"eq\":\n",
    "                                expr = F.col(column) == value\n",
    "                            elif comparison == \"ne\":\n",
    "                                expr = ((F.col(column) != value) | (F.col(column).isNull()) )\n",
    "                            else:\n",
    "                                if self._verbose_logging:\n",
    "                                    self._logger.warning(f\"Unsupported comparison operator: {comparison} in {rule_name}\")\n",
    "\n",
    "                            if condition_expr is None:\n",
    "                                condition_expr = expr\n",
    "                            else:\n",
    "                                condition_expr = condition_expr & expr\n",
    "\n",
    "                    # Drop the email address to lowercase and create a new column using the condition if applicable\n",
    "                    if condition_expr is not None:\n",
    "                        formatted_df = formatted_df.withColumn(f\"{column_name}{suffix}\", \n",
    "                            F.when(condition_expr, F.lower(column_name))\n",
    "                            .otherwise(None))\n",
    "                    else:\n",
    "                        formatted_df = formatted_df.withColumn(f\"{column_name}{suffix}\", F.lower(column_name)) \n",
    "\n",
    "                    formatted_df = formatted_df.withColumn(\n",
    "                        \"error_message\",\n",
    "                        F.when(\n",
    "                            ~F.col(f\"{column_name}{suffix}\").rlike(email_regex),\n",
    "                            F.lit(email_error_message),\n",
    "                        ).when(\n",
    "                            F.col(f\"{column_name}{suffix}\").contains(\"test\"),\n",
    "                            \"Email address indicates this is a test row\",\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    formatted_df = self._log_failed_rows(data_df=formatted_df,\n",
    "                                                         validation=validation,\n",
    "                                                         rule_name=rule_name,\n",
    "                                                         validation_stage=\"row processing\")\n",
    "                        \n",
    "            case \"phone\":\n",
    "                validation = \"Format Phone\"\n",
    "\n",
    "                for column_name in columns:\n",
    "                    formatted_column_name = f\"{column_name.strip('`')}{suffix}\"\n",
    "\n",
    "                    # Get set error message if no custom metadata provided\n",
    "                    phone_error_message = f\"{column_name} must be in a standard telephone format\" if not error_message else error_message\n",
    "\n",
    "                    # In all outcomes of the operation remove '+' characters and whitespace\n",
    "                    #  and if the formatted column value starts with 07 then replace 07 with 447\n",
    "                    #  and if the column value starts with 7 then replace with 447\n",
    "                    #  then log as an error message if the string value is non numeric\n",
    "                    formatted_df = (\n",
    "                        formatted_df\n",
    "                        .withColumn(\n",
    "                            formatted_column_name,\n",
    "                            F.when(\n",
    "                                F.col(column_name).startswith(\"07\"),\n",
    "                                F.expr(f\"CONCAT('447', SUBSTRING(regexp_replace(`{column_name}`, '[ +]', ''), 3, LENGTH(`{column_name}`)))\")\n",
    "                            ).when(\n",
    "                                F.col(column_name).startswith(\"7\"),\n",
    "                                F.expr(f\"CONCAT('447', SUBSTRING(regexp_replace(`{column_name}`, '[ +]', ''), 2, LENGTH(`{column_name}`)))\")  \n",
    "                            ).otherwise(\n",
    "                                F.regexp_replace(F.col(column_name), \"[ +]\", \"\")\n",
    "                            ))\n",
    "                        .withColumn(\n",
    "                            \"error_message\",\n",
    "                            F.when(\n",
    "                                ~F.col(formatted_column_name).rlike(\"^[0-9]*$\"),\n",
    "                                F.lit(phone_error_message)\n",
    "                            ).otherwise(\n",
    "                                    F.lit(None)\n",
    "                            ))\n",
    "                        )\n",
    "                    \n",
    "                    formatted_df = self._log_failed_rows(data_df=formatted_df,\n",
    "                                                         validation=validation,\n",
    "                                                         rule_name=rule_name,\n",
    "                                                         validation_stage=\"row processing\")\n",
    "\n",
    "            case \"postcode\":\n",
    "                # Postcode formatting does not produce an error.\n",
    "                validation = \"Format Postcode\"\n",
    "\n",
    "                # Regular expression pattern for standard email address\n",
    "                postcode_regex = \"^[A-Z]{1,2}[0-9R][0-9A-Z]? [0-9][ABD-HJLNP-UW-Z]{2}$\"\n",
    "                for column_name in columns:\n",
    "                    # If the postcode conforms to the standard UK format then set the value as\n",
    "                    #  uppercase and add a space prior to the last three characters, but if the postcode\n",
    "                    #  does not conform to the expected format then make no transformations\n",
    "                    formatted_df = formatted_df.withColumn(\n",
    "                        \"temp_postcode\",\n",
    "                        F.expr(f\"CONCAT(UPPER(SUBSTRING(regexp_replace(`{column_name}`, ' ', ''), 1, LENGTH(regexp_replace(`{column_name}`, ' ', ''))-3)), ' ', UPPER(SUBSTRING(regexp_replace(`{column_name}`, ' ', ''), LENGTH(regexp_replace(`{column_name}`, ' ', ''))-2, 3)))\")\n",
    "                    ).withColumn(\n",
    "                        f\"{column_name}{suffix}\",\n",
    "                        F.when(\n",
    "                            ~F.col(\"temp_postcode\").rlike(postcode_regex),\n",
    "                            F.col(column_name)\n",
    "                        ).otherwise(\n",
    "                            F.col(\"temp_postcode\")\n",
    "                        )\n",
    "                    ).drop(\"temp_postcode\")\n",
    "\n",
    "            case \"date\":\n",
    "                validation = \"Format Date\"\n",
    "\n",
    "                for column_name in columns:\n",
    "                    # Get set error message if no custom metadata provided\n",
    "                    if format_pattern != 'any':\n",
    "                        date_error_message = f\"{column_name} must be in date format: {format_pattern}\" if not error_message else error_message\n",
    "                    else:\n",
    "                        date_error_message = f\"{column_name} must be in date format yyyy-MM-dd HH:mm:ss\" if not error_message else error_message\n",
    "\n",
    "                    formatted_column_name = f\"{column_name.strip('`')}{suffix}\"\n",
    "\n",
    "                    if format_pattern != \"any\":\n",
    "                        # Try to parse the date directly - let PySpark handle validation\n",
    "                        parsed_date = F.to_date(F.col(column_name), format_pattern)\n",
    "\n",
    "                    else:\n",
    "                        valid_formats = [\n",
    "                            \"dd/MM/yyyy HH:mm:ss\",\n",
    "                            \"dd/MM/yyyy HH:mm\",\n",
    "                            \"dd/MM/yy\", \n",
    "                            \"dd/MM/yyyy\",\n",
    "                            \"yyyy-MM-dd HH:mm:ss\",\n",
    "                            \"yyyy-MM-dd HH:mm\"\n",
    "                        ]\n",
    "\n",
    "                        # loop through valid formats\n",
    "                        parsed_date_attempts = [\n",
    "                            F.to_date(F.col(column_name), fmt) for fmt in valid_formats\n",
    "                        ]\n",
    "\n",
    "                        # use coalesce to return the first successful format\n",
    "                        parsed_date = F.coalesce(*parsed_date_attempts)\n",
    "\n",
    "                    formatted_df = (\n",
    "                        formatted_df\n",
    "                        .withColumn(\n",
    "                            formatted_column_name,\n",
    "                            F.when(\n",
    "                                (F.col(column_name).isNotNull()) & \n",
    "                                (F.col(column_name) != \"\") &\n",
    "                                (parsed_date.isNotNull()),\n",
    "                                parsed_date\n",
    "                            ).otherwise(F.lit(None))\n",
    "                        )\n",
    "                        .withColumn(\n",
    "                            \"error_message\",\n",
    "                            F.when(\n",
    "                                (F.col(column_name).isNotNull()) & \n",
    "                                (F.col(column_name) != \"\") &\n",
    "                                (parsed_date.isNull()),\n",
    "                                F.lit(date_error_message)\n",
    "                            ).otherwise(F.lit(None))\n",
    "                        )\n",
    "                    )\n",
    "        \n",
    "                    formatted_df = self._log_failed_rows(data_df=formatted_df,\n",
    "                                                         validation=validation,\n",
    "                                                         rule_name=rule_name,\n",
    "                                                         validation_stage=\"row processing\")\n",
    "\n",
    "            case \"datetime\":\n",
    "                validation = \"Format Datetime\"\n",
    "\n",
    "                for column_name in columns:\n",
    "                    if format_pattern != \"any\":\n",
    "                        # Get set error message if no custom metadata provided\n",
    "                        datetime_error_message = f\"{column_name} must be in date format: {format_pattern}\" if not error_message else error_message\n",
    "                    else:\n",
    "                        datetime_error_message = f\"{column_name} must be in date format yyyy-MM-dd HH:mm:ss\" if not error_message else error_message\n",
    "                    formatted_column_name = f\"{column_name.strip('`')}{suffix}\"\n",
    "\n",
    "                    if format_pattern != \"any\":\n",
    "                        # Try to parse the date directly - let PySpark handle validation\n",
    "                        parsed_date = F.to_timestamp(F.col(column_name), format_pattern)\n",
    "\n",
    "                    else:\n",
    "                        valid_formats = [\n",
    "                            \"dd/MM/yyyy HH:mm:ss\",\n",
    "                            \"dd/MM/yyyy HH:mm\",\n",
    "                            \"dd/MM/yy\", \n",
    "                            \"dd/MM/yyyy\",\n",
    "                            \"yyyy-MM-dd HH:mm:ss\",\n",
    "                            \"yyyy-MM-dd HH:mm\"\n",
    "                        ]\n",
    "\n",
    "                        # loop through valid formats\n",
    "                        parsed_date_attempts = [\n",
    "                            F.to_timestamp(F.col(column_name), fmt) for fmt in valid_formats\n",
    "                        ]\n",
    "\n",
    "                        # use coalesce to return the first successful format\n",
    "                        parsed_date = F.coalesce(*parsed_date_attempts)\n",
    "\n",
    "                    formatted_df = (\n",
    "                        formatted_df\n",
    "                        .withColumn(\n",
    "                            formatted_column_name,\n",
    "                            F.when(\n",
    "                                (F.col(column_name).isNotNull()) & \n",
    "                                (F.col(column_name) != \"\") &\n",
    "                                (parsed_date.isNotNull()),\n",
    "                                parsed_date\n",
    "                            ).otherwise(F.lit(None))\n",
    "                        )\n",
    "                        .withColumn(\n",
    "                            \"error_message\",\n",
    "                            F.when(\n",
    "                                (F.col(column_name).isNotNull()) & \n",
    "                                (F.col(column_name) != \"\") &\n",
    "                                (parsed_date.isNull()),\n",
    "                                F.lit(datetime_error_message)\n",
    "                            ).otherwise(F.lit(None))\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    formatted_df = self._log_failed_rows(data_df=formatted_df,\n",
    "                                                         validation=validation,\n",
    "                                                         rule_name=rule_name,\n",
    "                                                         validation_stage=\"row processing\")\n",
    "\n",
    "            case _:\n",
    "                if self._verbose_logging:\n",
    "                    self._logger.warning(f\"Received unexpected format '{format_name}' - must be one of email/phone/postcode/date/datetime\")                    \n",
    "                \n",
    "        return formatted_df\n",
    "    \n",
    "    def _concat_column_values(self, source_data_df: DataFrame, columns: list[str], output_column: str, sep: str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"Concatenates column values together into a new column.\n",
    "        Accepts a dataframe, a list of strings and a separator character and \n",
    "        creates a new column by concatenating the values of the supplied columns.\n",
    "        The values are separated by the optional separator character and the new column \n",
    "        is named from the output_column parameter. If all the input columns are null, a null is returned\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark dataframe containing the ingested file.\n",
    "            columns (list[str]): List of strings containing required column names.\n",
    "            output_column (str): Name of the column to create.\n",
    "            sep (str): Separator character to use when concatenating column values.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            \n",
    "        Returns:\n",
    "            concat_df (DataFrame): The dataframe with the additional derived column.\n",
    "        \"\"\"\n",
    "        # Concatenate the column values with the supplied separator\n",
    "        concat_df = source_data_df.withColumn(output_column, F.concat_ws(sep, *columns))\n",
    "        \n",
    "        # Replace output column with null if it is equal to empty string, such as when concatting all null columns\n",
    "        concat_df = concat_df.withColumn(output_column, F.when(F.col(output_column) != \"\", F.col(output_column)).otherwise(F.lit(None)))\n",
    "\n",
    "        return concat_df    \n",
    "\n",
    "    def _copy_columns(self, source_data_df: DataFrame, columns: list[dict]) -> DataFrame:\n",
    "        \"\"\"Copies specified column values into new columns.\n",
    "        Iterates a list of column names and extracts the values in the specified columns \n",
    "        into new derived columns with the required name.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark dataframe containing the ingested data.\n",
    "            columns (list[dict]): A list of dictionaries containing the columns\n",
    "                                    to extract from and new columns to create.\n",
    "            \n",
    "            Example columns value:\n",
    "            [{\n",
    "                \"column\": \"FName\",\n",
    "                \"new_column_name\": \"NewFName\"\n",
    "            },\n",
    "            {\n",
    "                \"column\": \"SName\",\n",
    "                \"new_column_name\": \"NewLName\"\n",
    "            }]\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            \n",
    "        Returns:\n",
    "            new_cols_df (DataFrame): Transformed data including the new columns.\n",
    "        \"\"\"\n",
    "        # Iterate the copy columns assigning the existing column value to the new name\n",
    "        for item in columns:\n",
    "            # Create a new column with the same values as the original column\n",
    "            copied_cols_df = source_data_df.withColumn(item[\"new_column_name\"], F.col(item[\"column\"]))        \n",
    "\n",
    "        return copied_cols_df\n",
    "\n",
    "    def _set_default_column_value(self, data_df: DataFrame, columns: list[str], default_value: Any, suffix: str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"Creates new defaulted column with values if null.\n",
    "        For each column name specified in the columns list, a new column is \n",
    "        created in the supplied dataframe taking the value of the existing column \n",
    "        but if the value is null then the default_value supplied is used. The new\n",
    "        column is named from the existing column name with the value of the suffix\n",
    "        parameter appended to the string e.g. FirstName_defaulted.\n",
    "        If the column does not exist in the dataset then create a new column column_suffix \n",
    "        with the value\n",
    "\n",
    "        Args:\n",
    "            data_df (DataFrame): Spark dataframe too add the columns to.\n",
    "            columns (list[str]): List of column names to process.\n",
    "            default_value (str): Default value to assign if none exists.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            \n",
    "        Returns:\n",
    "            data_df (DataFrame): Spark dataframe containing the default values.\n",
    "        \"\"\"\n",
    "        for column_name in columns:\n",
    "            if column_name in data_df.columns:\n",
    "                data_df = (\n",
    "                    data_df\n",
    "                    .withColumn(f\"{column_name}{suffix}\",\n",
    "                                F.when((F.col(column_name).isNull()) | (F.col(column_name) == \"\"), default_value)\n",
    "                                .otherwise(F.col(column_name)))\n",
    "                    )\n",
    "            else:\n",
    "                data_df = (\n",
    "                    data_df\n",
    "                    .withColumn(f\"{column_name}{suffix}\", F.lit(default_value))\n",
    "                    )\n",
    "\n",
    "        \n",
    "        return data_df\n",
    "\n",
    "    def _remove_characters(self, data_df: DataFrame, characters: list[str], column_name: str, output_column: str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"Creates a copy of supplied columns removing required characters.\n",
    "        Accepts a list of characters and column name and creates a new column\n",
    "        named from the output_name parameter containing the existing contents\n",
    "        of the column. Then proceeds to remove all occurances of characters in\n",
    "        the characters parameter preserving the original value. If after removing\n",
    "        all characters, only an empty string remains, a null is returned in that cell.\n",
    "\n",
    "        Args:\n",
    "            data_df (DataFrame): Spark dataframe to transform.\n",
    "            characters (list[str]): List of characters to remove.\n",
    "            column_name (str): Column name to transform.\n",
    "            output_column (str): Name of the created column.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "\n",
    "        Returns:\n",
    "            data_df (DataFrame): Spark dataframe containing the transformed column.\n",
    "        \"\"\"\n",
    "        # Create a new column with the same values as the original column  \n",
    "        data_df = data_df.withColumn(output_column, F.col(column_name))  \n",
    "    \n",
    "        # Apply regexp_replace function to the new column in the loop\n",
    "        for character in characters:\n",
    "            data_df = data_df.withColumn(output_column,\n",
    "                                         F.regexp_replace(F.col(output_column), character, \"\")\n",
    "                                         )\n",
    "\n",
    "        # After removing characters if only an empty string is remaining, this is set to null.\n",
    "        data_df = data_df.withColumn(output_column,\n",
    "                                     F.when(F.col(output_column) != \"\", F.col(output_column))\n",
    "                                     .otherwise(F.lit(None))\n",
    "                                     )\n",
    "\n",
    "        return data_df\n",
    "    \n",
    "    def _backdate_four_tax_years(self, data_df: DataFrame, backdate_column:str, output_column_name: str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"Backdates a given backdate_column to 1st April and subtract 4 years.\n",
    "\n",
    "        Args:\n",
    "            data_df (DataFrame) : Spark dataframe to transform.\n",
    "            backdate_column (str) : Column to backdate. MUST be a date column.\n",
    "            output_column_name (str) : Newly backdated column\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            \n",
    "        Returns:\n",
    "            backdated_df (DataFrame) where the column has been backdated to 1st April four years ago.\n",
    "\n",
    "        Eg:\n",
    "            +---------------------+--------------+\n",
    "            |Starting Date_as_date|backdated_date|\n",
    "            +---------------------+--------------+\n",
    "            |           2024-12-01|    2020-04-01|\n",
    "            |           2025-05-01|    2021-04-01|\n",
    "            |           2025-02-01|    2020-04-01|\n",
    "            +---------------------+--------------+\n",
    "        \"\"\"\n",
    "        # Calculate the tax year base year\n",
    "        backdated_df = (\n",
    "            data_df\n",
    "            .withColumn(\"base_year\", \n",
    "                F.when(F.month(F.col(backdate_column)) < 4, \n",
    "                    F.year(F.col(backdate_column)) - 5)\n",
    "                .otherwise(F.year(F.col(backdate_column))-4)\n",
    "            )\n",
    "            .withColumn(output_column_name, F.concat_ws(\"-\",\n",
    "                F.col(\"base_year\"), F.lit(\"04\"), F.lit(\"01\")))\n",
    "        )\n",
    "\n",
    "        return backdated_df\n",
    "    \n",
    "    def _add_date_part(self, source_data_df: DataFrame, date_column: str, date_part: str, output_column: str, error_message: str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"Creates a new column by extracting the required date part from column values.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): The dataframe to transform.\n",
    "            date_column (str): The name of the date column to transform.\n",
    "            date_part (str): The part of the date to extract (day/month/year)\n",
    "            output_column (str): The name of the column to create.\n",
    "            error_message (str): Optional custom error message.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "\n",
    "        Returns:\n",
    "            check_date_df (DataFrame): Transformed dataframe.\n",
    "        \"\"\"\n",
    "        validation = \"Date Part\"\n",
    "        \n",
    "        if date_part == \"year\":\n",
    "            check_date_df = (\n",
    "                source_data_df\n",
    "                .withColumn(output_column,\n",
    "                    F.when(\n",
    "                        F.col(date_column).isNotNull(),\n",
    "                        F.year(F.col(date_column))\n",
    "                    ).otherwise(F.lit(None))\n",
    "                )\n",
    "            )\n",
    "        elif date_part == \"month\":\n",
    "            check_date_df = (\n",
    "                source_data_df\n",
    "                .withColumn(output_column,\n",
    "                    F.when(\n",
    "                        F.col(date_column).isNotNull(),\n",
    "                        F.month(F.col(date_column))\n",
    "                    ).otherwise(F.lit(None))\n",
    "                )\n",
    "            )\n",
    "        elif date_part == \"day\":\n",
    "            check_date_df = (\n",
    "                source_data_df\n",
    "                .withColumn(output_column,\n",
    "                    F.when(\n",
    "                        F.col(date_column).isNotNull(),\n",
    "                        F.dayofmonth(F.col(date_column))\n",
    "                    ).otherwise(F.lit(None))\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            if self._verbose_logging:\n",
    "                self._logger.warning(f\"unexpected date_part {date_part} was supplied\")\n",
    "\n",
    "        error_rows_df = check_date_df.where((F.col(output_column).isNull()) & (F.col(date_column).isNotNull()))\n",
    "\n",
    "        self._log_failed_rows(error_rows_df, validation, rule_name, \"row processing\", error_message)\n",
    "\n",
    "\n",
    "        return check_date_df  \n",
    " \n",
    "    def _convert_values(self, source_data_df: DataFrame, columns: list, replaces: list[dict], suffix: str, error_message: str, rule_name:str, behaviour: str) -> DataFrame:\n",
    "        \"\"\"Creates copies of columns replacing existing values with supplied values.\n",
    "        For each column in the list supplied, a new column is created named from the\n",
    "        existing column name suffixed with the suffix parameter. The values in the\n",
    "        new column are then replaced by the values in the replaces parameter. If the\n",
    "        behaviour parameter is set to \"keep_others\" then any value not replaced will be\n",
    "        retained as is, if the behaviour is \"null_others\" then values not replaced will\n",
    "        be replaced by a null value, and if the behavious is \"fail_row\" which is the\n",
    "        default then the error will be logged.\n",
    "    \n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark dataframe of source data.\n",
    "            columns (list): List of column names to perform the operation against.\n",
    "            replaces (list[dict]) : List of dictionary of from and to values to convert.\n",
    "            suffix (str): The suffix of the new column name.\n",
    "            error_message (str): An optional custom error message.\n",
    "            rule_name (str): Name of the metadata rule that called this function\n",
    "            behaviour (str): Specifies how to handle values not replaced. Has a default value\n",
    "                            of \"keep_others\" allowing for no value to be supplied and no \n",
    "                            action taken.\n",
    "    \n",
    "        Returns:\n",
    "            source_data_df (DataFrame): Spark dataframe with new columns.\n",
    "        \"\"\"\n",
    "        from_values = [key[\"from\"] for key in replaces]\n",
    "        validation = \"Convert Values\"\n",
    "        \n",
    "        if behaviour:\n",
    "            behaviour = behaviour.get(\"if_other_values_found\")\n",
    "        else:\n",
    "            behaviour = \"keep_others\"\n",
    "\n",
    "        for column in columns:\n",
    "            convert_error_message = error_message if error_message else f\"{column} is blank or contains a value that is not allowed. Replace it with one of the following: \" + \", \".join(from_values)\n",
    "\n",
    "            new_column = f\"{column}{suffix}\"\n",
    "            source_data_df = source_data_df.withColumn(new_column, F.lit(None))\n",
    "\n",
    "            for replace in replaces:\n",
    "                if behaviour == \"keep_others\":\n",
    "                    source_data_df = (\n",
    "                        source_data_df\n",
    "                        .withColumn(new_column,\n",
    "                                    F.when(\n",
    "                                        (F.lower(F.col(column)) == F.lower(F.lit(replace[\"from\"])))\n",
    "                                        & (F.col(new_column).isNull()),\n",
    "                                        replace[\"to\"])\n",
    "                                    .otherwise(F.col(new_column))\n",
    "                                    )\n",
    "                                )\n",
    "                elif behaviour == \"null_others\":\n",
    "                    source_data_df = (                    \n",
    "                        source_data_df\n",
    "                        .withColumn(new_column,\n",
    "                                    F.when(F.lower(F.col(column)) == F.lower(F.lit(replace[\"from\"])), replace[\"to\"])\n",
    "                                    .otherwise(F.col(new_column))\n",
    "                                )\n",
    "                            )\n",
    "                elif behaviour == \"fail_row\":\n",
    "                    source_data_df = (\n",
    "                        source_data_df\n",
    "                            .withColumn(new_column,\n",
    "                                    F.when(F.lower(F.col(column)) == F.lower(F.lit(replace[\"from\"])), replace[\"to\"])\n",
    "                                    .otherwise(F.col(new_column)) # Retain the original value if no match\n",
    "                                )\n",
    "                            )\n",
    "            \n",
    "            # If behaviour is 'null_others' then set any unaccepted value to null        \n",
    "            if behaviour == \"null_others\":\n",
    "                valid_values = [item[\"to\"] for item in replaces]\n",
    "\n",
    "                source_data_df = source_data_df.withColumn(new_column, \n",
    "                    F.when(F.col(new_column).isin(valid_values), F.col(new_column))\n",
    "                    .otherwise(F.lit(None))\n",
    "                    )\n",
    "            elif behaviour == \"keep_others\":\n",
    "                source_data_df = (\n",
    "                    source_data_df\n",
    "                    .withColumn(new_column,\n",
    "                                F.when(F.col(new_column).isNull(), F.col(column))\n",
    "                                .otherwise(F.col(new_column))\n",
    "                                )\n",
    "                            )                   \n",
    "            elif behaviour == \"fail_row\":            \n",
    "                valid_values = [item[\"to\"] for item in replaces] \n",
    "                errors_df = source_data_df.where(~F.col(new_column).isin(valid_values) | F.col(new_column).isNull())\n",
    "                errors_df = errors_df.withColumn(\"error_message\", F.lit(convert_error_message))\n",
    "                \n",
    "                self._log_failed_rows(errors_df, validation, rule_name, \"row processing\")\n",
    "\n",
    "        \n",
    "        return source_data_df\n",
    "    \n",
    "    \n",
    "    def _create_field(self, source_data_df: DataFrame, value:str, function:str, output:str, rule_name:str, conditions: list[str]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a column in the data frame with a value from value or a function\n",
    "        Args:\n",
    "            source_data_df: DataFrame: Spark dataframe containing source data\n",
    "            value(str): The value to assign to the column\n",
    "            function(str): The function from which to get a value to assign to a column. Valid values are get_date and get_timestamp\n",
    "            output:str): The name of the column to create \n",
    "            rule_name(str): The name of the rule from metadata, used for logging if required\n",
    "            conditions(list): An optional list of conditions for rows where the field should be created.\n",
    "        Returns:\n",
    "            source_data_df (DataFrame): Spark dataframe with new columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iterate over each of the conditions in condition to create an expression function\n",
    "        condition_expr = None\n",
    "        if conditions:\n",
    "            for condition in conditions:\n",
    "                column = condition[\"column\"]\n",
    "                comparison = condition[\"comparison\"]\n",
    "                con_value = condition['value']\n",
    "\n",
    "                if comparison == \"eq\":\n",
    "                    expr = F.col(column) == con_value\n",
    "                elif comparison == \"ne\":\n",
    "                    expr = ((F.col(column) != value) | (F.col(column).isNull()) )\n",
    "                else:\n",
    "                    if self._verbose_logging:\n",
    "                        self._logger.warning(f\"Unsupported comparison operator: {comparison} in {rule_name}\")\n",
    "\n",
    "                if condition_expr is None:\n",
    "                    condition_expr = expr\n",
    "                else:\n",
    "                    condition_expr = condition_expr & expr\n",
    "\n",
    "        # If there are no conditions, then set the Condition expression to a column expression that always\n",
    "        # evaluates to True. That way the update will be applied to all rows\n",
    "        if condition_expr is None:\n",
    "            condition_expr = F.lit(True)\n",
    "\n",
    "        # Define the function that will be called if it is on a function\n",
    "        if function:\n",
    "            match function:\n",
    "                case \"get_date\": \n",
    "                    field_value = F.lit(datetime.now().date())\n",
    "                case \"get_timestamp\":\n",
    "                    field_value = F.lit(datetime.now())\n",
    "                case \"create_guid\":\n",
    "                    field_value = F.expr(\"uuid()\")\n",
    "                case _:\n",
    "                    if self._verbose_logging:\n",
    "                        self._logger.warning(f\"Unsupported create_field operator: {function}\")\n",
    "\n",
    "        else:\n",
    "            field_value = F.lit(value)\n",
    "\n",
    "        \n",
    "        # Create the output by returning a column with field_value when the conditions are all true, otherwise a null value.\n",
    "        create_df = source_data_df.withColumn(\n",
    "            output,\n",
    "            F.when(condition_expr, field_value)\n",
    "            .otherwise(None))\n",
    "\n",
    "        return create_df\n",
    "    \n",
    "    def _first_non_null(self, source_data_df: DataFrame, columns:list, output:str, rule_name:str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Gets the first value that is not null from a list of columns\n",
    "        Args:\n",
    "            source_data_df: DataFrame: Spark dataframe containing source data\n",
    "            columns(list): A list of the columns \n",
    "            output:(str): The name of the column to create \n",
    "            rule_name(str): The name of the rule from metadata, used for logging if required\n",
    "            \n",
    "        Returns:\n",
    "            source_data_df (DataFrame): Spark dataframe with new columns.\n",
    "        \"\"\"\n",
    "        coalesce_df = source_data_df.withColumn(output, F.coalesce(*[F.when(F.col(column) != \"\", F.col(column)).otherwise(None) for column in columns]))\n",
    "        \n",
    "\n",
    "        return coalesce_df \n",
    "    \n",
    "    def _create_conditional_column(self, source_data_df: DataFrame, if_thens: list[dict], else_value: str, output: str, rule_name: str) -> DataFrame:\n",
    "        \"\"\"Creates a column with values depending on the supplied conditions.\n",
    "\n",
    "        Args:\n",
    "            source_data_df: DataFrame: Spark dataframe containing source data\n",
    "            if_thens(list[dict]): A list of the conditions held in a dictionary with column, comparison, and value fields.\n",
    "                                e.g. \n",
    "                                        \"if_thens\": [\n",
    "                                            {\n",
    "                                                \"conditions\": [\n",
    "                                                {\n",
    "                                                    \"column\": \"Donor FirstName\",\n",
    "                                                    \"comparison\": \"ne\",\n",
    "                                                    \"value\": \"Anonymous\"\n",
    "                                                },\n",
    "                                                {\n",
    "                                                    \"column\": \"Donation Consent\",\n",
    "                                                    \"comparison\": \"eq\",\n",
    "                                                    \"value\": \"Yes\"\n",
    "                                                }\n",
    "                                            ],\n",
    "                                            \"then\": true\n",
    "                                            }\n",
    "                                        ],\n",
    "            else_value: The value to assign if the conditions are not met.\n",
    "            output:(str): The name of the column to create .\n",
    "            rule_name(str): The name of the rule from metadata, used for logging if required.\n",
    "\n",
    "        Returns:\n",
    "            source_data_df (DataFrame): Spark dataframe with new column.\n",
    "        \"\"\"\n",
    "        conditional_df = source_data_df.withColumn(output, F.lit(None))\n",
    "\n",
    "        expr = None\n",
    "        for if_then in if_thens:\n",
    "            then_value = if_then.get(\"then\")\n",
    "            then_column = if_then.get(\"then_source\")\n",
    "            conditions = if_then.get(\"conditions\", [])\n",
    "            cond_expr = None\n",
    "\n",
    "            # Iterate the conditions to construct a PySpark filter expression\n",
    "            for condition in conditions:\n",
    "                column = condition[\"column\"]\n",
    "                comparison = condition[\"comparison\"]\n",
    "                value = condition.get(\"value\")\n",
    "                if comparison == \"eq\":\n",
    "                    this_expr = F.col(column) == value\n",
    "                elif comparison == \"ne\":\n",
    "                    this_expr = ((F.col(column) != value) | (F.col(column).isNull()) )\n",
    "                else:\n",
    "                    this_expr = F.lit(False)\n",
    "                    if self._verbose_logging:\n",
    "                        self._logger.warning(f\"Unsupported comparison operator: {comparison} in {rule_name}\")\n",
    "                \n",
    "                cond_expr = this_expr if cond_expr is None else (cond_expr & this_expr)\n",
    "\n",
    "            # Determine the value to use: either a literal or a column reference\n",
    "            if then_column:\n",
    "                value_expr = F.col(then_column)\n",
    "            else:\n",
    "                value_expr = F.lit(then_value)\n",
    "\n",
    "            # Append the current expression to the when chain\n",
    "            expr = F.when(cond_expr, value_expr) if expr is None else expr.when(cond_expr, value_expr)\n",
    "        \n",
    "        # Add an otherwise clause to apply if the conditions are not met\n",
    "        expr = expr.otherwise(else_value)\n",
    "\n",
    "        conditional_df = conditional_df.withColumn(output, expr)\n",
    "\n",
    "        return conditional_df\n",
    "    \n",
    "    def _remove_whitespace(self, source_data_df: DataFrame, column: str, how: str) -> DataFrame:\n",
    "        \"\"\"Removes whitespace from a given column in a DataFrame.\n",
    "        The function removes whitespace from the column specified in the column parameter.\n",
    "        The how parameter determines whether whitespace is removed from both ends of the string,\n",
    "        just the left end, or just the right end.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): The DataFrame containing the column to process.\n",
    "            column (str): The name of the column from which to remove whitespace.\n",
    "            how (str): Specifies how to remove whitespace. Options are 'both', 'left', or 'right'.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: The DataFrame with whitespace removed from the specified column.\n",
    "        \"\"\"\n",
    "        match how:\n",
    "            case \"both\":\n",
    "                trimmed_df = source_data_df.withColumn(column, F.trim(F.col(column)))\n",
    "            case \"left\":\n",
    "                trimmed_df = source_data_df.withColumn(column, F.ltrim(F.col(column)))\n",
    "            case \"right\":\n",
    "                trimmed_df = source_data_df.withColumn(column, F.rtrim(F.col(column)))\n",
    "            case _:\n",
    "                if self._verbose_logging:\n",
    "                    self._logger.warning(f\"Unsupported whitespace removal option: {how}. No changes made to column {column}.\")\n",
    "                trimmed_df = source_data_df\n",
    "\n",
    "        return trimmed_df\n",
    "\n",
    "    # Custom validation\n",
    "    def custom_validation(self, source_data_df: DataFrame, metadata_rules: dict) -> bool:\n",
    "        \"\"\"Performs custom validations on a given DataFrame based on provided metadata rules.\n",
    "        The supplied dictionary 'metadata_rules' should contain a 'rules' key which \n",
    "        is a list of dictionaries where each dictionary represents a validation rule. The rules \n",
    "        are iterated sequentially and separate validation functions called to validate different \n",
    "        requirements. If a file is deemed invalid then the function breaks out of the iteration and\n",
    "        returns a boolean of true or false along with the supplied dataframe.\n",
    "\n",
    "        Each validation rule should contain 'type', 'name' and optionally 'custom_error_code'.\n",
    "        'type' is the type of validation rule and could be 'expect_column_values_to_be_unique_in_file', \n",
    "        'expect_column_values_to_not_be_null_custom' or 'expect_column_values'. 'name' is the name of the \n",
    "        validation rule. 'custom_error_code' is the custom error message for the validation rule.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (pandas.DataFrame): The DataFrame on which the validations are to be performed.\n",
    "            metadata_rules (dict): A dictionary containing validation rules.\n",
    "        \n",
    "        Returns:\n",
    "            is_file_valid (bool): True if the DataFrame is valid based on the validation rules, False otherwise.\n",
    "            source_data_df (DataFrame): The DataFrame after the validations have been performed.\n",
    "        \"\"\"       \n",
    "        is_file_valid = True\n",
    "        \n",
    "        if metadata_rules is None:\n",
    "            return is_file_valid, source_data_df\n",
    "\n",
    "        rules = metadata_rules.get(\"rules\")\n",
    "\n",
    "        for rule in rules:\n",
    "            validation_type = rule.get(\"type\")\n",
    "            rule_name = rule.get(\"name\")\n",
    "            custom_error_code = rule.get(\"custom_error_code\", None)\n",
    "            \n",
    "            match validation_type:\n",
    "                case \"expect_column_values_to_be_unique_in_file\":\n",
    "                    column_name = rule.get(\"column\")\n",
    "                    action = rule.get(\"action\")\n",
    "                    error_message = f\"{column_name} contains duplicate values\" if not custom_error_code else custom_error_code\n",
    "\n",
    "                    is_file_valid = self._expect_column_values_to_be_unique_in_file(source_data_df,\n",
    "                                                                                    column_name,\n",
    "                                                                                    action,\n",
    "                                                                                    error_message,\n",
    "                                                                                    rule_name)\n",
    "                    \n",
    "                case \"expect_column_values_to_not_be_null_custom\":\n",
    "                    columns = rule.get(\"columns\")\n",
    "                    conditions = rule.get(\"conditions\")\n",
    "                    action = rule.get(\"action\")\n",
    "                    default_custom_error_code = rule.get(\"default_custom_error_code\")\n",
    "                    is_file_valid = self._expect_column_values_to_not_be_null_custom(source_data_df=source_data_df,\n",
    "                                                                                     columns=columns,\n",
    "                                                                                     conditions=conditions,\n",
    "                                                                                     default_error_code=default_custom_error_code,\n",
    "                                                                                     rule_name=rule_name,\n",
    "                                                                                     action=action)\n",
    "                    \n",
    "                case \"expect_column_values\":\n",
    "                    conditions = rule.get(\"conditions\")\n",
    "                    action = rule.get(\"action\")\n",
    "                    rule_name = rule.get(\"name\")\n",
    "\n",
    "                    source_data_df, is_file_valid = self._expect_column_values(source_data_df=source_data_df,\n",
    "                                                                               conditions=conditions,\n",
    "                                                                               action=action,\n",
    "                                                                               rule_name=rule_name)\n",
    "                    \n",
    "                case \"_\":\n",
    "                    print(f\"Received unexpected validation type '{validation_type}'\")\n",
    "            \n",
    "            # Exit the loop if the file is not valid\n",
    "            if not is_file_valid:\n",
    "                return is_file_valid, source_data_df\n",
    "\n",
    "\n",
    "        return is_file_valid, source_data_df\n",
    "    \n",
    "    def _expect_column_values_to_be_unique_in_file(self, source_data_df: DataFrame, column_name: str, action: str, error_message: str, rule_name:str) -> bool:\n",
    "        \"\"\"Checks a data source does not have duplicates.\n",
    "        The function checks for duplicates in the values in the column_name parameter. \n",
    "        If any rows in the supplied Dataframe have duplicate data then the errors are logged for each column.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark dataset containing the rows to validate.\n",
    "            column_name (str): The name of the column to validate.\n",
    "            action: The action to take in the event of duplicates. Options expected reject_file or reject_row. Default is reject_row.\n",
    "            error_message (str): An optional custom error message which is only used if provided.\n",
    "            rule_name (str): Name of the metadata rule that called this function.\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns True or False to denote the validity of the file.\n",
    "        \"\"\"    \n",
    "        duplicates_df = source_data_df.groupBy(column_name).count().where(F.col(\"count\") > 1)\n",
    "        validation = \"Unique Value\"\n",
    "\n",
    "        if not duplicates_df.isEmpty():\n",
    "            # Set an error message if error not provided\n",
    "            if not error_message:\n",
    "                error_message = \"Duplicate value in \" + column_name\n",
    "\n",
    "            # Handle errors in file or row\n",
    "            if action == \"reject_file\":\n",
    "                return False\n",
    "            else:\n",
    "                # Add duplicate rows to a failed data frame\n",
    "                failed_rows_df = source_data_df.join(duplicates_df, on=source_data_df[column_name] == duplicates_df[column_name], how=\"inner\")\n",
    "                failed_rows_df = failed_rows_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "                self._log_failed_rows(failed_rows_df, validation, rule_name, \"custom validation\")\n",
    "        \n",
    "        # Return true to mark the file as not rejected\n",
    "        return True\n",
    "    \n",
    "    def _expect_column_values_to_not_be_null_custom(self, source_data_df: DataFrame, columns: list, conditions:list[dict], default_error_code: str, rule_name: str, action: str) -> bool:\n",
    "        \"\"\"Checks that the column value is not null, and supplies a specific error message depending on values in other fields\n",
    "        Args:\n",
    "            source_data_df (DataFrame): Spark dataset containing the rows to validate.\n",
    "            columns (list): The name of the columns to validate.\n",
    "            action: The action to take in the event of duplicates. Options expected reject_file or reject_row. Default is reject_row\n",
    "            error_message (str): An optional custom error message which is only used if provided.\n",
    "            rule_name (str): Name of the metadata rule that called this function.\n",
    "\n",
    "        Returns:\n",
    "            bool\n",
    "        \"\"\"    \n",
    "        validation = \"Expect Not Null\"\n",
    "        errors_df = self._spark.createDataFrame([], schema=source_data_df.schema)\n",
    "        \n",
    "        for column in columns:\n",
    "            error_df = source_data_df.where(F.col(column).isNull())\n",
    "            errors_df = errors_df.union(error_df)\n",
    "            \n",
    "        if not errors_df.isEmpty():\n",
    "            errors_df = errors_df.withColumn(\"Error\", F.lit(None))\n",
    "\n",
    "            for condition in conditions:\n",
    "                condition_expr = None\n",
    "                \n",
    "                column = condition[\"column\"]\n",
    "                comparison = condition[\"comparison\"]\n",
    "                value = condition[\"value\"]\n",
    "                custom_error_code = condition[\"custom_error_code\"]\n",
    "\n",
    "                if comparison == \"eq\":\n",
    "                    expr = F.col(column) == value\n",
    "                elif comparison == \"ne\":\n",
    "                    expr = ((F.col(column) != value) | (F.col(column).isNull()) )\n",
    "                else:\n",
    "                    self._logger.warning(f\"Unsupported comparison operator: {comparison} in {rule_name}\")\n",
    "\n",
    "                condition_expr = expr            \n",
    "                errors_df = errors_df.withColumn(\"Error\", F.when(condition_expr, custom_error_code).otherwise(F.col(\"Error\")))\n",
    "            \n",
    "            errors_df = errors_df.withColumn(\"Error\", F.when(F.col(\"Error\").isNull(), default_error_code).otherwise(F.col(\"Error\")))\n",
    "\n",
    "            if action == \"reject_row\":\n",
    "                self._log_failed_rows(errors_df, validation, rule_name, \"custom validation\")\n",
    "            elif action == \"reject_file\":\n",
    "                self._log_failed_rows(errors_df, validation, rule_name, \"custom validation\")\n",
    "                return False\n",
    "\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _expect_column_values(self, source_data_df: DataFrame, conditions: list[dict], action: str, rule_name: str) -> DataFrame:\n",
    "        \"\"\"Conditionally performs actions such as dropping rows.\n",
    "        The function accepts values parsed from metadata detailing conditional operations \n",
    "        to perform against the given dataframe. The specified action is only applied to \n",
    "        rows which meet the condition.\n",
    "\n",
    "        Args:\n",
    "            source_data_df (DataFrame): The DataFrame on which the operations are to be performed.\n",
    "            conditions (list[dict]): A list of dictionaries where each dictionary represents a condition. Each dictionary \n",
    "                                    should contain 'column', 'comparison' and 'value' keys. 'column' is the column in the DataFrame \n",
    "                                    where the condition is to be applied. 'comparison' is the comparison operator and could be 'eq' (equal),\n",
    "                                    'ne' (not equal), 'lt' (less than), 'gt' (greater than), 'isnull' (is null), 'notnull' (is not null). \n",
    "                                    'value' is the value for the comparison.\n",
    "            action (str): The action to be performed if the condition is met. Currently, only 'drop_row' action is supported which\n",
    "                        drops the row if the condition is met.\n",
    "            rule_name (str): The name of the rule extracted from the metadata.\n",
    "        \n",
    "        Returns:\n",
    "            source_data_df (DataFrame): Processed dataframe.\n",
    "        \"\"\"\n",
    "        for condition in conditions:\n",
    "            column = condition.get(\"column\")\n",
    "            comparison = condition.get(\"comparison\")\n",
    "            value = condition.get(\"value\")\n",
    "            \n",
    "            if comparison == \"eq\":\n",
    "                filter_condition = F.col(column) == value\n",
    "                comparison_name = \"equal\"\n",
    "            elif comparison == \"ne\":\n",
    "                filter_condition = ((F.col(column) != value) | (F.col(column).isNull()) )\n",
    "                comparison_name = \"different\"\n",
    "            elif comparison == \"lt\":\n",
    "                filter_condition = F.col(column) < value\n",
    "                comparison_name = \"less than\"\n",
    "            elif comparison == \"gt\":\n",
    "                filter_condition = F.col(column) > value\n",
    "                comparison_name = \"greater than\"\n",
    "            elif comparison == \"isnull\":\n",
    "                filter_condition = F.col(column).isNull()\n",
    "                comparison_name = \"null\"\n",
    "            elif comparison == \"notnull\":\n",
    "                filter_condition = F.col(column).isNotNull()\n",
    "                comparison_name = \"not null\"\n",
    "            else:\n",
    "                print(f\"Recieved unexpected comparison operator of '{comparison}'\")\n",
    "            \n",
    "            error_message = f\"Column '{column}' should be {comparison_name} to '{value}'\"\n",
    "            \n",
    "            if action == \"drop_row\":\n",
    "                rowcount = source_data_df.count()\n",
    "                \n",
    "                # Apply filter\n",
    "                filtered_df = source_data_df.where(filter_condition)\n",
    "\n",
    "                # Remove filtered_df rows from source_data_df\n",
    "                source_data_df = source_data_df.join(filtered_df, on=[\"row_id\"], how=\"left_anti\")\n",
    "\n",
    "                new_rowcount = source_data_df.count()\n",
    "                dropped_rowcount = rowcount - new_rowcount\n",
    "            elif action == \"fail_file\":\n",
    "                filtered_df = source_data_df.where(filter_condition)\n",
    "                errors_df = filtered_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "                self._log_failed_rows(errors_df, \"expect_column_values\", rule_name, \"custom validation\")\n",
    "\n",
    "                return source_data_df, False\n",
    "            elif action == \"reject_row\":\n",
    "                filtered_df = source_data_df.where(filter_condition)\n",
    "                errors_df = filtered_df.withColumn(\"error_message\", F.lit(error_message))\n",
    "                self._log_failed_rows(errors_df, \"expect_column_values\", rule_name, \"custom validation\")\n",
    "            else:\n",
    "                print(f\"Recieved unexpected action operator of '{action}'\")\n",
    "\n",
    "                return source_data_df, False\n",
    "\n",
    "        return source_data_df, True"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
